# -*- coding: utf-8 -*-
"""Building a LDA topic model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1weCvHtWm4XBjSuNUQeHLfHb33QgoYeFk

# (1) Importing Text
"""

from google.colab import files
uploaded = files.upload()

"""Installing docx2txt library to read MS Word Documents

"""

pip install docx2txt

import docx2txt

ca = docx2txt.process('/content/Client A.docx')
cb = docx2txt.process('/content/Client B.docx')
cc = docx2txt.process('/content/Client C.docx')
cd = docx2txt.process('/content/Client D.docx')
ce = docx2txt.process('/content/Client E.docx')
cf = docx2txt.process('/content/Client F.docx')
cg = docx2txt.process('/content/Client G.docx')
ch = docx2txt.process('/content/Client H.docx')
ci = docx2txt.process('/content/Client I.docx')
cj = docx2txt.process('/content/Client J.docx')

ci

print(len(cg))

print('Type of variable produced by Txt file read approach..', (type(cb)))

"""# (2) Data Cleaning"""

list(ca)

"""Can see the string variables are all complied together, and when presented in a list (which will be requried further down for pre-processing) will be read by character rather than word."""

# Using split function to seperate string variables into words rather than characters.

str_ca = ca.split()
str_cb = cb.split()
str_cc = cc.split()
str_cd = cd.split()
str_ce = ce.split()
str_cf = cf.split()
str_cg = cg.split()
str_ch = ch.split()
str_ci = ci.split()
str_cj = cj.split()

list(str_cj)

str_ch

"""# (3) Data Pre-Processing

Pre-processing to include:


*   Tokenisation,
*   Removing punctuation & unnecessary characters,
*   Remove Stopwords,
*   Creating Bigrams & Trigrams,
*   Lemmatization.
"""

# Importing libaries
import re
import numpy as np
import pandas as pd
from pprint import pprint

# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel

# spacy for lemmatization
import spacy

# # Remove distracting single quotes

# # data = [re.sub("\n", "", sent) for sent in data]

# str_ca = [re.sub("\n", "", sent) for sent in str_ca]
# str_cb = [re.sub("\n", "", sent) for sent in str_cb]
# str_cc = [re.sub("\n", "", sent) for sent in str_cc]
# str_cd = [re.sub("\n", "", sent) for sent in str_cd]
# str_ce = [re.sub("\n", "", sent) for sent in str_ce]
# str_cf = [re.sub("\n", "", sent) for sent in str_cf]
# str_cg = [re.sub("\n", "", sent) for sent in str_cg]
# str_ch = [re.sub("\n", "", sent) for sent in str_ch]
# str_ci = [re.sub("\n", "", sent) for sent in str_ci]
# str_cj = [re.sub("\n", "", sent) for sent in str_cj]

my_stop_words_list = (['until', 'every', 'seems', 'became', 'into', 'don', 'back', 'many', 'others', 'as', 'hereafter', 'my', 'being', 'whom', 'onto', 'has', 'besides', 'after', 'otherwise', 'km', 'from', 'beyond', 'whether', 'everyone', 'then', 'would', 'same', 'because', 'show', 'above', 'under', 'over', 'such', 'everything', 're',
                       'yourself', 'is', 'yet', 'four', 'bottom', 'could', 'thereby', 'amongst', 'nowhere', 'or', 'used', 'us', 'next', 'within', 'not', 'hundred', 'never', 'something', 'herein', 'latter', 'meanwhile', 'eight', 'name', 'an', 'i', 'thru', 'while', 'unless', 'doing', 'no',
                       'once', 'anyhow', 'cant', 'what', 'eleven', 'among', 'just', 'un', 'least', 'here', 'by', 'during', 'mostly', 'were', 'hereby', 'hers', 'itself', 'a', 'however', 'done', 'whose', 'please', 'beforehand', 'de', 'except', 'when', 'much', 'have', 'so', 'both', 'regarding', 'any', 'former', 'me', 'eg', 'whatever',
                       'than', 'etc', 'go', 'several', 'each', 'about', 'off', 'per', 'whenever', 'say', 'kg', 'via', 'made', 'for', 'call', 'whoever', 'anything', 'becomes', 'with', 'sixty', 'move', 'am', 'be', 'two', 'themselves', 'was', 'fifty', 'whereas', 'and', 'cry', 'across', 'fifteen', 'too', 'does', 'against', 'thick', 'it',
                       'all', 'hereupon', 'sometimes', 'fire', 'might', 'one', 'anyone', 'put', 'seemed', 'sometime', 'thus', 'well', 'below', 'still', 'indeed', 'will', 'mill', 'third', 'towards', 'make', 'if', 'throughout', 'nothing', 'some', 'serious', 'everywhere', 'due', 'since',
                       'are', 'three', 'forty', 'behind', 'beside', 'had', 'none', 'noone', 'side', 'twelve', 'upon', 'didn', 'been', 'namely', 'almost', 'whither', 'must', 'their', 'though', 'in', 'he', 'somehow', 'at', 'seeming', 'often', 'although', 'on', 'more', 'through', 'elsewhere', 'full', 'nevertheless', 'her', 'around',
                       'which', 'before', 'those', 'yours', 'other', 'anywhere', 'most', 'therefore', 'your', 'perhaps', 'you', 'who', 'thin', 'quite', 'wherein', 'own', 'either', 'do', 'really', 'also', 'alone', 'where', 'ours', 'doesn', 'somewhere', 'latterly', 'neither', 'of', 'always', 'between', 'find', 'herself', 'becoming',
                       'further', 'inc', 'they', 'did', 'become', 'that', 'thence', 'she', 'the', 'hasnt', 'another', 'therein', 'very', 'toward', 'front', 'various', 'already', 'anyway', 'first', 'get', 'someone', 'down', 'whereafter', 'may', 'to', 'six', 'whence', 'out', 'hence', 'amount',
                       'whereupon', 'con', 'wherever', 'ourselves', 'thereupon', 'see', 'else', 'using', 'enough', 'five', 'we', 'take', 'these', 'yourselves', 'few', 'twenty', 'give', 'ever', 'should', 'whole', 'but', 'there', 'sincere', 'moreover', 'along', 'whereby', 'mine', 'them', 'our',
                       'up', 'now', 'its', 'without', 'thereafter', 'found', 'himself', 'nobody', 'describe', 'ie', 'nine', 'myself', 'why', 'less', 'even', 'afterwards', 'only', 'keep', 'rather', 'formerly', 'ten', 'seem', 'his', 'last', 'together', 'fill', 'bill', 'co', 'this', 'him', 'again',
                       'amongst', 'ltd', 'can', 'nor', 'ict', 'client', 'audit', 'peers', 'auditors', 'consultants', 'organisation', 'business', 'cipfa', 'appendices', 'introduction', 'recommendations', 'contents', 'report', 'review', 'summary', '\n', 'internal', 'future', 'objective', 'priority',
                       'achieve', 'how', 'change', 'deliver', 'description', 'page', 'articulate', 'peer', 'item', 'section', 'include', 'fundamental', 'view', 'detail', 'work', 'current', 'help', 'provided', 'content', 'draft', 'similar', 'use', 'annual', 'assessment', 'corporate', 'impact', 'mention',
                       'year', 'note', 'control', 'identify', 'january', 'febuary', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'background', 'key', 'findings', 'areas', 'action', 'plan', 'strategy', 'guidance', 'appointed', 'conducted', 'carried',
                       'part', 'approved', 'committee', 'detailed', 'scope', 'set', 'grateful', 'improvement', 'high', 'level', 'provide', 'result', 'matter', 'come', 'attention', 'example', 'feedback', 'discuss', 'relevant', 'aim', 'ensure', 'period', 'emerge', 'manage', 'good', 'challenge', 'analysis', 'partially', 'item', 'group', 'implement', 'adequate', 'consider',
                       'agree', 'follow','inphase', 'note', 'activity', 'party', 'share', 'adequacy', 'initiative', 'location', 'comparison', 'require', 'work', 'exist', 'datum', 'local', 'approval', 'arise', 'stakeholder', 'head', 'job', 'theme', 'category', 'definition', 'expect', 'practice', 'include',
                       'development', 'place', 'achievement', 'require', 'effectiveness', 'consistent', 'document', 'agreed', 'project', 'responsibility', 'operate', 'develop', 'observation', 'follow', 'appropriate', 'emerge', 'consider', 'reference', 'role', 'mitigate', 'goal', 'know', 'addition', 'status',
                       'reports', 'prepared', 'company', 'llp', 'request', 'terms', 'raised', 'came', 'whilst', 'matters', 'care', 'take', 'preparation', 'accurate', 'included', 'objectives', 'supported', 'delivery', 'plans', 'aims', 'modification', 'modifications', 'mentioned', 'significant', 'strategic',
                       'initiatives', 'organisational', 'cover', 'covered', 'provided', 'necessarily', 'previous', 'activities', 'items', 'operate', 'operated', 'plans', 'controls', 'maintain', 'maintained', 'involve', 'involved', 'effectively', 'occur', 'occurs', 'consider', 'considers', 'considered', 'understand',
                       'understanding', 'specific', 'specifics', 'gap', 'assess', 'assesses', 'challenge', 'strength', 'weakness', 'reduce', 'reduced', 'sector', 'sectors', 'risk', 'risks', 'support', 'supported', 'supports', 'follow', 'follows', 'statement', 'statements', 'include', 'included', 'includes', 'definition',
                       'definitions', 'good', 'require', 'requires', 'capability', 'capabilities', 'expect', 'expects', 'strategy', 'strategic', 'strategies', 'improvement', 'party', 'emerges', 'extent', 'extents', 'activities', 'activity', 'cover', 'covers', 'operate', 'operated', 'operates', 'mitigate', 'mitigates', 'item',
                       'items', 'takes', 'take', 'promote', 'promotes', 'component', 'components', 'framework', 'frameworks', 'conclusion', 'conclusions', 'implements', 'implement', 'know', 'knows', 'full', 'benefit', 'benefits', 'weaknesses', 'strengths', 'required', 'themes', 'theme', 'themed', 'assess', 'assessed', 'extract',
                       'extracts', 'extracted', 'supports', 'support', 'supported', 'reason', 'reasons', 'reasoned', 'weaknesses', 'add', 'adds', 'added', 'complete', 'completed', 'completes', 'well', 'wells', 'add', 'adds', 'added', 'work', 'works', 'worked', 'knowed', 'known', 'explored', 'explores', 'explore', 'share', 'shares', 'shared',
                       'follows', 'followed', 'provides', 'provided', 'provide', 'permit', 'permits', 'permited', 'consequently', 'progress', 'progresses', 'progressed', 'entirely', 'emerged', 'emerges', 'change', 'changed', 'changes', 'expects', 'expected', 'parties', 'partys', 'issue', 'issues', 'issued', 'nature', 'disclaim', 'disclaims',
                       'goods', 'accept', 'notes', 'noted', 'note', 'whatsoever', 'taken', 'steer', 'senior', 'equally', 'easily', 'breakdown', 'actual', 'appendix', 'potential', 'years', 'assurance', 'core', 'values', 'past', 'history', 'present', 'depreciation', 'shifting', 'effect', 'total', 'enabling', 'lives', 'live', 'lived', 'homes',
                       'council', 'force', 'conjunction', 'read', 'emerging', 'executive', 'vision', 'im', 'captured', 'end', 'disclaimer', 'principles', 'roadmap', 'state', 'general', 'imperatives', 'target', 'chapter', 'industry', 'developments', 'policy', 'represent', 'represents', 'single', 'table', 'tables', 'advancements', 'advancement', 'coherent',
                       'reporting', 'reports', 'reported', 'led', 'empower', 'empowers', 'empowered', 'new', 'old', 'overview', 'overviews', 'continuous', 'outcomes', 'harnessing', 'delivered', 'provision', 'profile', 'able', 'manner', 'base', 'value', 'values', 'valued', 'ago', 'experience', 'experiences', 'align', 'alignment', 'aligns', 'trigger',
                       'triggers', 'triggered', 'unit', 'phase', 'given', 'guarantee', 'comprehensive', 'relies', 'rely', 'relied', 'replacement', 'replacements', 'replaced', 'organisations', 'organisation', 'error', 'errors', 'subsequently', 'perform', 'performed', 'performs', 'level', 'levels', 'leveled', 'document', 'documents', 'documented', 'liability',
                       'liabilitys', 'liabilities', 'oversight', 'oversights', 'assumption', 'assumptions', 'assumed', 'overall', 'specifically', 'submit', 'submits', 'good', 'goods', 'efficiency', 'efficiencies', 'accept', 'accepts', 'accepted', 'recognise', 'recognises', 'recognised', 'full', 'fulls', 'fulled', 'housekeeping', 'unnecessary', 'provide', 'provides', 'provided',
                       'sign', 'signs', 'signed', 'irregularity', 'irregularities', 'confirm', 'confirmed', 'confirms', 'likely', 'likes', 'liked', 'response', 'responses', 'responded', 'open', 'opens', 'opened', 'exist', 'exists', 'well', 'child', 'identify', 'currently', 'implement', 'implements', 'implemented', 'currently', 'implement', 'implements', 'implemented', 'propose',
                       'proposed', 'proposes', 'develop', 'develops', 'developed', 'limitation', 'limitations', 'limited', 'limits', 'planning', 'planned', 'plans', 'plan', 'select', 'selects', 'selected', 'stage', 'stages', 'staged', 'intend', 'intends', 'intended', 'subject', 'subjects', 'subjected', 'permit', 'permits', 'permited', 'example', 'examples', 'ongoing', 'ongoings', 'review',
                       'reviews', 'reviewed', 'expose', 'exposes', 'exposed', 'relate', 'relates', 'related', 'basis', 'based', 'way', 'ways', 'highlight', 'highlights', 'discuss', 'discussed', 'discusses', 'forward', 'design', 'designs', 'lesson', 'review', 'reviews', 'reviewed', 'collusive', 'go', 'regular', 'budget', 'remain', 'remains', 'fraud', 'circumstance', 'circumstances',
                       'ready', 'formally', 'different', 'order', 'orders', 'focus', 'reproduce', 'reproduced', 'create', 'created', 'creates', 'project', 'projects', 'stakeholder', 'stakeholders', 'selection', 'selections', 'overlap', 'overlaps', 'duplication', 'duplicated', 'duplicates', 'create', 'accordingly', 'arrangement', 'arrangements', 'degree', 'degrees', 'minor', 'minors', 'purpose', 'purposes', 'choice', 'choices', 'casework', 'separate', 'separated', 'separates',
                       'goals', 'goal', 'reference', 'references', 'generally', 'understands', 'understand', 'articulate', 'articulates', 'articulated', 'heavily', 'police', 'interview', 'interviews', 'offer', 'offers', 'offered', 'help', 'helped', 'helps', 'engagement', 'engagements', 'establish', 'established', 'drive', 'drives', 'drived', 'drivers', 'demand', 'demanded', 'demands', 'purpose', 'apply', 'applies', 'record', 'recorded', 'records', 'start', 'starts', 'started', 'end', 'finish',
                       'priority', 'point', 'points', 'indicates', 'fully', 'trent', 'dove', 'poor', 'up', 'board', 'boards', 'move', 'moves', 'moved', 'ensure', 'ensures', 'have', 'not', 'own', 'option', 'options', 'look', 'looks', 'line', 'lines', 'see', 'saw', 'inconsistent', 'inconsistently', 'publish', 'published', 'reduce', 'reduces', 'define', 'defines', 'allow', 'allowed', 'wide', 'narrow', 'aspiration', 'aspire', 'keep', 'lose', 'identify', 'improve', 'achieve', 'continue', 'imperative', 'outline', 'draw', 'essential', 'additional',
                       'strive', 'expectation', 'expectations', 'check', 'checks', 'checked', 'deliver', 'delivers', 'contact', 'influence', 'influenced', 'make', 'long', 'longer', 'intervention', 'lack', 'lacks', 'age', 'ages', 'aged', 'highlighted', 'context', 'including', 'rapidly', 'on_page', 'to_date', 'diagnosis', 'optimise', 'channel', 'decade', 'link', 'begin', 'clearly', 'aligned', 'following', 'followed', 'follows', 'follower', 'purports', 'purport', 'purpose', 'measures', 'measure', 'conducted', 'conducted', 'conduct', 'conducts', 'conducting', 'deliver', 'delivered', 'delivering',
                       'relate', 'related', 'relating', 'relates', 'improvements', 'improvement', 'improve', 'improves', 'need', 'needs', 'needing', 'sound', 'reasonable', 'recomendation', 'recomendations', 'identifies', 'identify', 'identified', 'discussion', 'discussions', 'external', 'internal', 'focus', 'focused', 'loss', 'won', 'permitted', 'proof', 'confidential', 'reassess', 'assessment', 'fullest', 'step', 'steps', 'substantial', 'course', 'rest', 'major', 'approach', 'approaches', 'possible', 'landscape', 'indication', 'indicates', 'amendment', 'amendments', 'result', 'results', 'resulting', 'inappropriate', 'substitute', 'substitutes', 'clear', 'unclear', 'sample', 'determine', 'determined', 'determining', 'enable',
                       'absolute', 'designing', 'disclosed', 'disclosure', 'disclaims', 'consultation', 'underpinning', 'under', 'underpinned'])

"""**Tokenise & punctuation**

Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.

Gensim’s simple_preprocess() is great for this. Additionally I have set deacc=True to remove the punctuations.
"""

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

# running tokenise function

ca_words = list(sent_to_words(str_ca))
cb_words = list(sent_to_words(str_cb))
cc_words = list(sent_to_words(str_cc))
cd_words = list(sent_to_words(str_cd))
ce_words = list(sent_to_words(str_ce))
cf_words = list(sent_to_words(str_cf))
cg_words = list(sent_to_words(str_cg))
ch_words = list(sent_to_words(str_ch))
ci_words = list(sent_to_words(str_ci))
cj_words = list(sent_to_words(str_cj))

print(ch_words)

"""**Bigrams & trigrams**

Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring. Some examples in our example are: ‘front_bumper’, ‘oil_leak’, ‘maryland_college_park’ etc.

Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams.
"""

# Build the bigram and trigram models
bigram1 = gensim.models.Phrases(ca_words, min_count=1, threshold=1) # higher threshold fewer phrases.
trigram1 = gensim.models.Phrases(bigram1[ca_words], threshold=1)

bigram2 = gensim.models.Phrases(cb_words, min_count=1, threshold=1) # higher threshold fewer phrases.
trigram2 = gensim.models.Phrases(bigram2[cb_words], threshold=1)

bigram3 = gensim.models.Phrases(cc_words, min_count=1, threshold=1) # higher threshold fewer phrases.
trigram3 = gensim.models.Phrases(bigram3[cc_words], threshold=1)

bigram4 = gensim.models.Phrases(cd_words, min_count=1, threshold=1) # higher threshold fewer phrases.
trigram4 = gensim.models.Phrases(bigram4[cd_words], threshold=1)

bigram5 = gensim.models.Phrases(ce_words, min_count=1, threshold=1) # higher threshold fewer phrases.
trigram5 = gensim.models.Phrases(bigram5[ce_words], threshold=1)

bigram6 = gensim.models.Phrases(cf_words, min_count=1, threshold=1) # higher threshold fewer phrases.
trigram6 = gensim.models.Phrases(bigram6[cf_words], threshold=1)

bigram7 = gensim.models.Phrases(cg_words, min_count=1, threshold=1) # higher threshold fewer phrases.
trigram7 = gensim.models.Phrases(bigram7[cg_words], threshold=1)

bigram8 = gensim.models.Phrases(ch_words, min_count=1, threshold=1) # higher threshold fewer phrases.
trigram8 = gensim.models.Phrases(bigram8[ch_words], threshold=1)

bigram9 = gensim.models.Phrases(ci_words, min_count=1, threshold=1) # higher threshold fewer phrases.
trigram9 = gensim.models.Phrases(bigram9[ci_words], threshold=1)

bigram10 = gensim.models.Phrases(cj_words, min_count=1, threshold=1) # higher threshold fewer phrases.
trigram10 = gensim.models.Phrases(bigram10[cj_words], threshold=1)


# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod1 = gensim.models.phrases.Phraser(bigram1)
trigram_mod1 = gensim.models.phrases.Phraser(trigram1)

bigram_mod2 = gensim.models.phrases.Phraser(bigram2)
trigram_mod2 = gensim.models.phrases.Phraser(trigram2)

bigram_mod3 = gensim.models.phrases.Phraser(bigram3)
trigram_mod3 = gensim.models.phrases.Phraser(trigram3)

bigram_mod4 = gensim.models.phrases.Phraser(bigram4)
trigram_mod4 = gensim.models.phrases.Phraser(trigram4)

bigram_mod5 = gensim.models.phrases.Phraser(bigram5)
trigram_mod5 = gensim.models.phrases.Phraser(trigram5)

bigram_mod6 = gensim.models.phrases.Phraser(bigram6)
trigram_mod6 = gensim.models.phrases.Phraser(trigram6)

bigram_mod7 = gensim.models.phrases.Phraser(bigram7)
trigram_mod7 = gensim.models.phrases.Phraser(trigram7)

bigram_mod8 = gensim.models.phrases.Phraser(bigram8)
trigram_mod8 = gensim.models.phrases.Phraser(trigram8)

bigram_mod9 = gensim.models.phrases.Phraser(bigram9)
trigram_mod9 = gensim.models.phrases.Phraser(trigram9)

bigram_mod10 = gensim.models.phrases.Phraser(bigram10)
trigram_mod10 = gensim.models.phrases.Phraser(trigram10)

# See trigram example
print(trigram_mod1[bigram_mod1[cb_words[0]]])

"""**Remove Stopwords, & run Make Bigrams and Lemmatize**

The bigrams model is ready. Let’s define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially.
"""

# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
# python3 -m spacy download en
nlp = spacy.load('en', disable=['parser', 'ner'])

# Define functions for stopwords, bigrams, trigrams and lemmatization
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in my_stop_words_list] for doc in texts]



# Define functions for bigrams

def make_bigrams1(texts):
    return [bigram_mod1[doc] for doc in texts]

def make_bigrams2(texts):
    return [bigram_mod2[doc] for doc in texts]

def make_bigrams3(texts):
    return [bigram_mod3[doc] for doc in texts]

def make_bigrams4(texts):
    return [bigram_mod4[doc] for doc in texts]

def make_bigrams5(texts):
    return [bigram_mod5[doc] for doc in texts]

def make_bigrams6(texts):
    return [bigram_mod6[doc] for doc in texts]

def make_bigrams7(texts):
    return [bigram_mod7[doc] for doc in texts]

def make_bigrams8(texts):
    return [bigram_mod8[doc] for doc in texts]

def make_bigrams9(texts):
    return [bigram_mod9[doc] for doc in texts]

def make_bigrams10(texts):
    return [bigram_mod10[doc] for doc in texts]

# Define functions for trigrams

def make_trigrams1(texts):
    return [trigram_mod1[bigram_mod1[doc]] for doc in texts]

def make_trigrams2(texts):
    return [trigram_mod2[bigram_mod2[doc]] for doc in texts]

def make_trigrams3(texts):
    return [trigram_mod3[bigram_mod3[doc]] for doc in texts]

def make_trigrams4(texts):
    return [trigram_mod4[bigram_mod4[doc]] for doc in texts]

def make_trigrams5(texts):
    return [trigram_mod5[bigram_mod5[doc]] for doc in texts]

def make_trigrams6(texts):
    return [trigram_mod6[bigram_mod6[doc]] for doc in texts]

def make_trigrams7(texts):
    return [trigram_mod7[bigram_mod7[doc]] for doc in texts]

def make_trigrams8(texts):
    return [trigram_mod8[bigram_mod8[doc]] for doc in texts]

def make_trigrams9(texts):
    return [trigram_mod9[bigram_mod9[doc]] for doc in texts]

def make_trigrams10(texts):
    return [trigram_mod10[bigram_mod10[doc]] for doc in texts]


# Define functions for lemmatization

# def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
#     """https://spacy.io/api/annotation"""
#     texts_out = []
#     for sent in texts:
#         doc = nlp(" ".join(sent))
#         texts_out.append([token.lemma_ for token in doc if token.pos_ is allowed_postags])
#     return texts_out


def lemmatization(texts):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent))
        texts_out.append([token.lemma_ for token in doc if token.pos_ not in my_stop_words_list])
    return texts_out

# Form Bigrams

ca_bigrams = make_bigrams1(ca_words)
cb_bigrams = make_bigrams2(cb_words)
cc_bigrams = make_bigrams3(cc_words)
cd_bigrams = make_bigrams4(cd_words)
ce_bigrams = make_bigrams5(ce_words)
cf_bigrams = make_bigrams6(cf_words)
cg_bigrams = make_bigrams7(cg_words)
ch_bigrams = make_bigrams8(ch_words)
ci_bigrams = make_bigrams9(ci_words)
cj_bigrams = make_bigrams10(cj_words)

print(ca_bigrams)
print(cb_bigrams)
print(cc_bigrams)
print(cd_bigrams)
print(ce_bigrams)
print(cf_bigrams)
print(cg_bigrams)
print(ch_bigrams)
print(ci_bigrams)
print(cj_bigrams)

ca_trigrams = make_trigrams1(ca_bigrams)
cb_trigrams = make_trigrams2(cb_bigrams)
cc_trigrams = make_trigrams3(cc_bigrams)
cd_trigrams = make_trigrams4(cd_bigrams)
ce_trigrams = make_trigrams5(ce_bigrams)
cf_trigrams = make_trigrams6(cf_bigrams)
cg_trigrams = make_trigrams7(cg_bigrams)
ch_trigrams = make_trigrams8(ch_bigrams)
ci_trigrams = make_trigrams9(ci_bigrams)
cj_trigrams = make_trigrams10(cj_bigrams)

print(ca_trigrams)
print(cb_trigrams)
print(cc_trigrams)
print(cd_trigrams)
print(ce_trigrams)
print(cf_trigrams)
print(cg_trigrams)
print(ch_trigrams)
print(ci_trigrams)
print(cj_trigrams)

# Remove Stop Words

ca_nostops = remove_stopwords(ca_trigrams)
cb_nostops = remove_stopwords(cb_trigrams)
cc_nostops = remove_stopwords(cc_trigrams)
cd_nostops = remove_stopwords(cd_trigrams)
ce_nostops = remove_stopwords(ce_trigrams)
cf_nostops = remove_stopwords(cf_trigrams)
cg_nostops = remove_stopwords(cg_trigrams)
ch_nostops = remove_stopwords(ch_trigrams)
ci_nostops = remove_stopwords(ci_trigrams)
cj_nostops = remove_stopwords(cj_trigrams)

print(ca_nostops)
print(cb_nostops)
print(cc_nostops)
print(cd_nostops)
print(ce_nostops)
print(cf_nostops)
print(cg_nostops)
print(ch_nostops)
print(ci_nostops)
print(cj_nostops)

ch_nostops

# Do lemmatization keeping only noun, adj, vb, adv

# ca_lemmatized = lemmatization(ca_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
# cb_lemmatized = lemmatization(cb_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
# cc_lemmatized = lemmatization(cc_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
# cd_lemmatized = lemmatization(cd_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
# ce_lemmatized = lemmatization(ce_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
# cf_lemmatized = lemmatization(cf_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
# cg_lemmatized = lemmatization(cg_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
# ch_lemmatized = lemmatization(ch_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
# ci_lemmatized = lemmatization(ci_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
# cj_lemmatized = lemmatization(cj_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

ca_lemmatized = lemmatization(ca_nostops)
cb_lemmatized = lemmatization(cb_nostops)
cc_lemmatized = lemmatization(cc_nostops)
cd_lemmatized = lemmatization(cd_nostops)
ce_lemmatized = lemmatization(ce_nostops)
cf_lemmatized = lemmatization(cf_nostops)
cg_lemmatized = lemmatization(cg_nostops)
ch_lemmatized = lemmatization(ch_nostops)
ci_lemmatized = lemmatization(ci_nostops)
cj_lemmatized = lemmatization(cj_nostops)

print(ca_lemmatized)
print(cb_lemmatized)
print(cc_lemmatized)
print(cd_lemmatized)
print(ce_lemmatized)
print(cf_lemmatized)
print(cg_lemmatized)
print(ch_lemmatized)
print(ci_lemmatized)
print(cj_lemmatized)

print(len(ca_lemmatized))
print(len(ca_nostops))

print(len(cb_lemmatized))
print(len(cb_nostops))

print(len(cc_lemmatized))
print(len(cc_nostops))

print(len(cd_lemmatized))
print(len(cd_nostops))

print(len(ce_lemmatized))
print(len(ce_nostops))

print(len(cf_lemmatized))
print(len(cf_nostops))

print(len(cg_lemmatized))
print(len(cg_nostops))

print(len(ch_lemmatized))
print(len(ch_nostops))

# Removing blank spaces.

clean_ca = list(filter(None, ca_lemmatized))
clean_cb = list(filter(None, cb_lemmatized))
clean_cc = list(filter(None, cc_lemmatized))
clean_cd = list(filter(None, cd_lemmatized))
clean_ce = list(filter(None, ce_lemmatized))
clean_cf = list(filter(None, cf_lemmatized))
clean_cg = list(filter(None, cg_lemmatized))
clean_ch = list(filter(None, ch_lemmatized))
clean_ci = list(filter(None, ci_lemmatized))
clean_cj = list(filter(None, cj_lemmatized))

print(clean_ca[0:25])
print(clean_cb[0:25])
print(clean_cc[0:25])
print(clean_cd[0:25])
print(clean_ce[0:25])
print(clean_cf[0:25])
print(clean_cg[0:25])
print(clean_ch[0:25])
print(clean_ci[0:25])
print(clean_cj[0:25])

"""# (4) Creating & Running LDA Model

Create a dictionary, bag of words representation & LDA model.
"""

'''
Create a dictionary from 'processed_docs' containing the number of times a word appears
in the training set using gensim.corpora.Dictionary and call it 'dictionary'
'''

ca_dic = gensim.corpora.Dictionary(clean_ca)
cb_dic = gensim.corpora.Dictionary(clean_cb)
cc_dic = gensim.corpora.Dictionary(clean_cc)
cd_dic = gensim.corpora.Dictionary(clean_cd)
ce_dic = gensim.corpora.Dictionary(clean_ce)
cf_dic = gensim.corpora.Dictionary(clean_cf)
cg_dic = gensim.corpora.Dictionary(clean_cg)
ch_dic = gensim.corpora.Dictionary(clean_ch)
ci_dic = gensim.corpora.Dictionary(clean_ci)
cj_dic = gensim.corpora.Dictionary(clean_cj)

print(ca_dic)
print(cb_dic)
print(cc_dic)
print(cd_dic)
print(ce_dic)
print(cf_dic)
print(cg_dic)
print(ch_dic)
print(ci_dic)
print(cj_dic)

# '''
# OPTIONAL STEP
# Remove very rare and very common words:

# - words appearing less than 15 times
# - words appearing in more than 10% of all documents
# '''

# cb_dic.filter_extremes(no_below=7, keep_n= 100000)

# # cb_dic.filter_extremes(no_below=5, no_above=0.8, keep_n= 100000)

print(cb_dic)

"""**Gensim doc2bow**

doc2bow(document)

Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method.
"""

'''
Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many
words and how many times those words appear. Save this to 'bow_corpus'
'''

ca_bow = [ca_dic.doc2bow(doc) for doc in clean_ca]
cb_bow = [cb_dic.doc2bow(doc) for doc in clean_cb]
cc_bow = [cc_dic.doc2bow(doc) for doc in clean_cc]
cd_bow = [cd_dic.doc2bow(doc) for doc in clean_cd]
ce_bow = [ce_dic.doc2bow(doc) for doc in clean_ce]
cf_bow = [cf_dic.doc2bow(doc) for doc in clean_cf]
cg_bow = [cg_dic.doc2bow(doc) for doc in clean_cg]
ch_bow = [ch_dic.doc2bow(doc) for doc in clean_ch]
ci_bow = [ci_dic.doc2bow(doc) for doc in clean_ci]
cj_bow = [cj_dic.doc2bow(doc) for doc in clean_cj]

# Human readable format of corpus (term-frequency)
[[(ce_dic[id], freq) for id, freq in cp] for cp in cb_bow[0:300]]

"""**LDA Model**

We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.

Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.

chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes.

We will be running LDA using all CPU cores to parallelize and speed up model training.

Some of the parameters we will be tweaking are:

num_topics is the number of requested latent topics to be extracted from the training corpus. id2word is a mapping from word ids (integers) to words (strings). It is used to determine the vocabulary size, as well as for debugging and topic printing. workers is the number of extra processes to use for parallelization. Uses all available cores by default. alpha and eta are hyperparameters that affect sparsity of the document-topic (theta) and topic-word (lambda) distributions. We will let these be the default values for now(default value is 1/num_topics)

Alpha is the per document topic distribution.

High alpha: Every document has a mixture of all topics(documents appear similar to each other). Low alpha: Every document has a mixture of very few topics Eta is the per topic word distribution.

High eta: Each topic has a mixture of most words(topics appear similar to each other). Low eta: Each topic has a mixture of few words. passes is the number of training passes through the corpus. For example, if the training corpus has 50,000 documents, chunksize is 10,000, passes is 2, then online training is done in 10 updates:

1 documents 0-9,999
2 documents 10,000-19,999
3 documents 20,000-29,999

random_state ({np.random.RandomState, int}, optional) – Either a randomState object or a seed to generate one. Useful for reproducibility.
"""

# LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine
# lda_model = gensim.models.LdaModel(bow_corpus,
#                                    num_topics = 10,
#                                    id2word = dictionary,
#                                    passes = 50)

# LDA multicore
'''
Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'
'''
# LDA model 1

lda_1 =  gensim.models.LdaMulticore(ca_bow,
                                   num_topics = 16,
                                   id2word = ca_dic,
                                   passes = 10,
                                   workers = 2)

lda_2 =  gensim.models.LdaMulticore(cb_bow,
                                   num_topics = 16,
                                   id2word = cb_dic,
                                   passes = 10,
                                   workers = 2)

lda_3 =  gensim.models.LdaMulticore(cc_bow,
                                   num_topics = 16,
                                   id2word = cc_dic,
                                   passes = 10,
                                   workers = 2)

lda_4 =  gensim.models.LdaMulticore(cd_bow,
                                   num_topics = 16,
                                   id2word = cd_dic,
                                   passes = 10,
                                   workers = 2)

lda_5 =  gensim.models.LdaMulticore(ce_bow,
                                   num_topics = 16,
                                   id2word = ce_dic,
                                   passes = 10,
                                   workers = 2)

lda_6 =  gensim.models.LdaMulticore(cf_bow,
                                   num_topics = 16,
                                   id2word = cf_dic,
                                   passes = 10,
                                   workers = 2)

lda_7 =  gensim.models.LdaMulticore(cg_bow,
                                   num_topics = 16,
                                   id2word = cg_dic,
                                   passes = 10,
                                   workers = 2)

lda_8 =  gensim.models.LdaMulticore(ch_bow,
                                   num_topics = 16,
                                   id2word = ch_dic,
                                   passes = 10,
                                   workers = 2)

lda_9 =  gensim.models.LdaMulticore(ci_bow,
                                   num_topics = 16,
                                   id2word = ci_dic,
                                   passes = 10,
                                   workers = 2)

lda_10 =  gensim.models.LdaMulticore(cj_bow,
                                   num_topics = 16,
                                   id2word = cj_dic,
                                   passes = 10,
                                   workers = 2)



# LDA model 2

# cb_lda_2 = gensim.models.ldamodel.LdaModel(corpus=cb_bow,
#                                            id2word=cb_dic,
#                                            num_topics=10,
#                                            random_state=100,
#                                            update_every=1,
#                                            chunksize=100,
#                                            passes=10,
#                                            alpha='auto',
#                                            per_word_topics=True)

'''
For each topic, we will explore the words occuring in that topic and its relative weight
'''
for idx, topic in lda_1.print_topics(-1):
    print("Topic: {} \nWords: {}".format(idx, topic ))
    print("\n")

'''
For each topic, we will explore the words occuring in that topic and its relative weight
'''
for idx, topic in lda_2.print_topics(-1):
    print("Topic: {} \nWords: {}".format(idx, topic ))
    print("\n")

'''
For each topic, we will explore the words occuring in that topic and its relative weight
'''
for idx, topic in lda_3.print_topics(-1):
    print("Topic: {} \nWords: {}".format(idx, topic ))
    print("\n")

'''
For each topic, we will explore the words occuring in that topic and its relative weight
'''
for idx, topic in lda_4.print_topics(-1):
    print("Topic: {} \nWords: {}".format(idx, topic ))
    print("\n")

'''
For each topic, we will explore the words occuring in that topic and its relative weight
'''
for idx, topic in lda_5.print_topics(-1):
    print("Topic: {} \nWords: {}".format(idx, topic ))
    print("\n")

'''
For each topic, we will explore the words occuring in that topic and its relative weight
'''
for idx, topic in lda_6.print_topics(-1):
    print("Topic: {} \nWords: {}".format(idx, topic ))
    print("\n")

'''
For each topic, we will explore the words occuring in that topic and its relative weight
'''
for idx, topic in lda_7.print_topics(-1):
    print("Topic: {} \nWords: {}".format(idx, topic ))
    print("\n")

'''
For each topic, we will explore the words occuring in that topic and its relative weight
'''
for idx, topic in lda_8.print_topics(-1):
    print("Topic: {} \nWords: {}".format(idx, topic ))
    print("\n")

'''
For each topic, we will explore the words occuring in that topic and its relative weight
'''
for idx, topic in lda_9.print_topics(-1):
    print("Topic: {} \nWords: {}".format(idx, topic ))
    print("\n")

'''
For each topic, we will explore the words occuring in that topic and its relative weight
'''
for idx, topic in lda_10.print_topics(-1):
    print("Topic: {} \nWords: {}".format(idx, topic ))
    print("\n")

"""# (5) Visualisation"""

# Visualisation

#specific version for Google Collab
!pip install pyLDAvis==2.1.2

import pyLDAvis.gensim

pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(lda_1, ca_bow, ca_dic)

pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(lda_2, cb_bow, cb_dic)

pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(lda_3, cc_bow, cc_dic)

pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(lda_4, cd_bow, cd_dic)

pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(lda_5, ce_bow, ce_dic)

pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(lda_6, cf_bow, cf_dic)

pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(lda_7, cg_bow, cg_dic)

pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(lda_8, ch_bow, ch_dic)

pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(lda_9, ci_bow, ci_dic)

pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(lda_10, cj_bow, cj_dic)

"""# (6) Model Evaluation"""

# Compute Perplexity
print('\nPerplexity: ', lda_1.log_perplexity(ca_bow))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_1, texts=ca_lemmatized, dictionary=ca_dic, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Compute Perplexity
print('\nPerplexity: ', lda_2.log_perplexity(cb_bow))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_2, texts=cb_lemmatized, dictionary=cb_dic, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Compute Perplexity
print('\nPerplexity: ', lda_3.log_perplexity(cc_bow))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_3, texts=cc_lemmatized, dictionary=cc_dic, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Compute Perplexity
print('\nPerplexity: ', lda_4.log_perplexity(cd_bow))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_4, texts=cd_lemmatized, dictionary=cd_dic, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Compute Perplexity
print('\nPerplexity: ', lda_5.log_perplexity(ce_bow))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_5, texts=ce_lemmatized, dictionary=ce_dic, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Compute Perplexity
print('\nPerplexity: ', lda_6.log_perplexity(cf_bow))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_6, texts=cf_lemmatized, dictionary=cf_dic, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Compute Perplexity
print('\nPerplexity: ', lda_7.log_perplexity(cg_bow))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_7, texts=cg_lemmatized, dictionary=cg_dic, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Compute Perplexity
print('\nPerplexity: ', lda_8.log_perplexity(ch_bow))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_8, texts=ch_lemmatized, dictionary=ch_dic, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Compute Perplexity
print('\nPerplexity: ', lda_9.log_perplexity(ci_bow))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_9, texts=ci_lemmatized, dictionary=ci_dic, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Compute Perplexity
print('\nPerplexity: ', lda_10.log_perplexity(cj_bow))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_10, texts=cj_lemmatized, dictionary=cj_dic, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Client A model evalution

cv_coherence_model_lda_1 = gensim.models.CoherenceModel(model=lda_1, corpus=ca_bow, texts=ca_lemmatized, dictionary=ca_dic, coherence='c_v')

avg_coherence_cv_1 = cv_coherence_model_lda_1.get_coherence()

umass_coherence_model_lda_1 = gensim.models.CoherenceModel(model=lda_1, corpus=ca_bow, texts=ca_lemmatized, dictionary=ca_dic, coherence='u_mass')

avg_coherence_umass_1 = umass_coherence_model_lda_1.get_coherence()

perplexity_1 = lda_1.log_perplexity(ca_bow)

print(avg_coherence_cv_1)

print(avg_coherence_umass_1)

print(perplexity_1)

# Client B model evalution

cv_coherence_model_lda_2 = gensim.models.CoherenceModel(model=lda_2, corpus=cb_bow, texts=cb_lemmatized, dictionary=cb_dic, coherence='c_v')

avg_coherence_cv_2 = cv_coherence_model_lda_2.get_coherence()

umass_coherence_model_lda_2 = gensim.models.CoherenceModel(model=lda_2, corpus=cb_bow, texts=cb_lemmatized, dictionary=cb_dic, coherence='u_mass')

avg_coherence_umass_2 = umass_coherence_model_lda_2.get_coherence()

perplexity_2 = lda_2.log_perplexity(cb_bow)

print(avg_coherence_cv_2)
print(avg_coherence_umass_2)
print(perplexity_2)

# Client C model evalution

cv_coherence_model_lda_3 = gensim.models.CoherenceModel(model=lda_3, corpus=cc_bow, texts=cc_lemmatized, dictionary=cc_dic, coherence='c_v')

avg_coherence_cv_3 = cv_coherence_model_lda_3.get_coherence()

umass_coherence_model_lda_3 = gensim.models.CoherenceModel(model=lda_3, corpus=cc_bow, texts=cc_lemmatized, dictionary=cc_dic, coherence='u_mass')

avg_coherence_umass_3 = umass_coherence_model_lda_3.get_coherence()

perplexity_3 = lda_3.log_perplexity(cc_bow)

print(avg_coherence_cv_3)
print(avg_coherence_umass_3)
print(perplexity_3)

# Client D model evalution

cv_coherence_model_lda_4 = gensim.models.CoherenceModel(model=lda_4, corpus=cd_bow, texts=cd_lemmatized, dictionary=cd_dic, coherence='c_v')

avg_coherence_cv_4 = cv_coherence_model_lda_4.get_coherence()

umass_coherence_model_lda_4 = gensim.models.CoherenceModel(model=lda_4, corpus=cd_bow, texts=cd_lemmatized, dictionary=cd_dic, coherence='u_mass')

avg_coherence_umass_4 = umass_coherence_model_lda_4.get_coherence()

perplexity_4 = lda_4.log_perplexity(cd_bow)

print(avg_coherence_cv_4)
print(avg_coherence_umass_4)
print(perplexity_4)

# Client E model evalution

cv_coherence_model_lda_5 = gensim.models.CoherenceModel(model=lda_5, corpus=ce_bow, texts=ce_lemmatized, dictionary=ce_dic, coherence='c_v')

avg_coherence_cv_5 = cv_coherence_model_lda_5.get_coherence()

umass_coherence_model_lda_5 = gensim.models.CoherenceModel(model=lda_5, corpus=ce_bow, texts=ce_lemmatized, dictionary=ce_dic, coherence='u_mass')

avg_coherence_umass_5 = umass_coherence_model_lda_5.get_coherence()

perplexity_5 = lda_5.log_perplexity(ce_bow)

print(avg_coherence_cv_5)
print(avg_coherence_umass_5)
print(perplexity_5)

# Client F model evalution

cv_coherence_model_lda_6 = gensim.models.CoherenceModel(model=lda_6, corpus=cf_bow, texts=cf_lemmatized, dictionary=cf_dic, coherence='c_v')

avg_coherence_cv_6 = cv_coherence_model_lda_6.get_coherence()

umass_coherence_model_lda_6 = gensim.models.CoherenceModel(model=lda_6, corpus=cf_bow, texts=cf_lemmatized, dictionary=cf_dic, coherence='u_mass')

avg_coherence_umass_6 = umass_coherence_model_lda_6.get_coherence()

perplexity_6 = lda_6.log_perplexity(cf_bow)

print(avg_coherence_cv_6)
print(avg_coherence_umass_6)
print(perplexity_6)

# Client G model evalution

cv_coherence_model_lda_7 = gensim.models.CoherenceModel(model=lda_7, corpus=cg_bow, texts=cg_lemmatized, dictionary=cg_dic, coherence='c_v')

avg_coherence_cv_7 = cv_coherence_model_lda_7.get_coherence()

umass_coherence_model_lda_7 = gensim.models.CoherenceModel(model=lda_7, corpus=cg_bow, texts=cg_lemmatized, dictionary=cg_dic, coherence='u_mass')

avg_coherence_umass_7 = umass_coherence_model_lda_7.get_coherence()

perplexity_7 = lda_7.log_perplexity(cg_bow)

print(avg_coherence_cv_7)
print(avg_coherence_umass_7)
print(perplexity_7)

# Client H model evalution

cv_coherence_model_lda_8 = gensim.models.CoherenceModel(model=lda_8, corpus=ch_bow, texts=ch_lemmatized, dictionary=ch_dic, coherence='c_v')

avg_coherence_cv_8 = cv_coherence_model_lda_8.get_coherence()

umass_coherence_model_lda_8 = gensim.models.CoherenceModel(model=lda_8, corpus=ch_bow, texts=ch_lemmatized, dictionary=ch_dic, coherence='u_mass')

avg_coherence_umass_8 = umass_coherence_model_lda_8.get_coherence()

perplexity_8 = lda_8.log_perplexity(ch_bow)

print(avg_coherence_cv_8)
print(avg_coherence_umass_8)
print(perplexity_8)

# Client I model evalution

cv_coherence_model_lda_9 = gensim.models.CoherenceModel(model=lda_9, corpus=ci_bow, texts=ci_lemmatized, dictionary=ci_dic, coherence='c_v')

avg_coherence_cv_9 = cv_coherence_model_lda_9.get_coherence()

umass_coherence_model_lda_9 = gensim.models.CoherenceModel(model=lda_9, corpus=ci_bow, texts=ci_lemmatized, dictionary=ci_dic, coherence='u_mass')

avg_coherence_umass_9 = umass_coherence_model_lda_9.get_coherence()

perplexity_9 = lda_9.log_perplexity(ci_bow)

print(avg_coherence_cv_9)
print(avg_coherence_umass_9)
print(perplexity_9)

# Client j model evalution

cv_coherence_model_lda_10 = gensim.models.CoherenceModel(model=lda_10, corpus=cj_bow, texts=cj_lemmatized, dictionary=cj_dic, coherence='c_v')

avg_coherence_cv_10 = cv_coherence_model_lda_10.get_coherence()

umass_coherence_model_lda_10 = gensim.models.CoherenceModel(model=lda_10, corpus=cj_bow, texts=cj_lemmatized, dictionary=cj_dic, coherence='u_mass')

avg_coherence_umass_10 = umass_coherence_model_lda_10.get_coherence()

perplexity_10 = lda_10.log_perplexity(cj_bow)

print(avg_coherence_cv_10)
print(avg_coherence_umass_10)
print(perplexity_10)